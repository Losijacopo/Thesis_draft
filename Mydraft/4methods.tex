\chapter{Methods}
\label{chapter:methods}

A properly planned project requires the analysis of multiple methods and strategies that can be applied during its design. 
Therefore, this chapter displays, focusing on the multiple aspects of the entire project, the different techniques, encountered revising the broad literature accessible in this field, and the decisions that brought to the actual development of the algorithm, which will be thoroughly described in the chapter \ref{chapter:implementation}.\\
In this chapter, an extensive outline of the work made is not produced, though. 
Instead, the following sections focus on a concise, but anyway exhaustive, tracing of the methods available for a correct dense disparity estimation and for image data processing.
Concurrently, the choices made for the specific aspect of the algorithm are defined and explained.

\section{Standard disparity estimation methods}
\label{section:std-methods}

An extensive and broad analysis of the standard and the deep learning methods for dense disparity estimation has already been presented in chapter \ref{chapter:background}, where the review of the literature related to this Master's thesis project has been carried out.
Nevertheless, these sections focus on a different aspect of that analysis, which is correlated to the algorithms that turned out to be the most relevant for this work and to the features of some of the current machine learning based techniques, which appear to be applicable to this project.\\
Considering the standard algorithms for recovering a dense depth image from a stereo pair, the initial approach on which it was decided to focus on for the designing of the project refers to the Semi-Global Matching (SGM) stereo method developed by Hirschm\"{u}ller \cite{Hirschmuller2008}.
The decision of starting to concentrate on that evaluation of the stereo matching problem was mainly driven by the recent methods designed for dense disparity estimation.
As a matter of fact, most of the algorithms created in the last decade are based on the Hirschm\"{u}ller idea or, at least, they make reference to several aspects of the functions outlined in \cite{Hirschmuller2008}.
This fact is clearly comprehensible considering that the SGM method was classified as the best algorithm at the time of its publication and it is currently one of the top-ranked algorithm in terms of sub-pixel accuracy and computational time.
Hence, taking into account the benchmark data based on two of the most employed dataset, i.e. \cite{Scharstein2014} and \cite{geiger2013vision}, and after an initial general review of the literature publish in this field, it results that the top-performing algorithms use key features of SGM in their main pipeline. 
Moreover, it is worth to point out that the majority of the most accurate real time deep learning based algorithm tends to adopt the main functions of the SGM model, exploiting neural networks to ensure real time evaluations of the disparities.
Therefore, it was initially though that a reasonable option for the developing of an algorithm, which would work in real time providing accurate outcomes, would focus the attention of the Hirschm\"{u}ller method.\\
As already introduced in section \ref{sec:stereometh}, the SGM method combines the idea of the standard local based algorithms, where the main cost calculations are made over limited size windows, and global algorithm, for which the best pixel disparity estimation is based on a global energy function. 
Thus, mainly because its hierarchical cost calculation, SGM method outperformed the top-ranked algorithm over the dataset \cite{scharstein2003high}, proving real-time performance of that dataset images. 
However, with the current dataset, such as Middlebury 2014 \cite{Scharstein2014} and KITTI \cite{geiger2013vision}, \cite{menze2015object}, the standard Semi-Global Matching algorithm provide optimal results in terms of accuracy but it tends to slightly suffer if computational time is considered.
For this reason, the most recent real time methods for dense disparity estimation establish the cost computation part of their pipeline over the SGM idea, although, they exploit hardware efficiency or even neural network structures to reach extremely fast computations.\\
Thus, considering this project, it was decided to initially build up the main pipeline on the SGM method structure and employ the stereo device designed by the company to reach real time performances, without loosing in accuracy. 
Furthermore, as previously anticipated, an upstream choice was made. 
As a matter of fact, the current top-performing algorithms strongly depend on convolutional and deep neural network to achieve fast computations.
Contrarily to that, we decided to based the skeleton of our method on the data available through the stereo system to reach a real time implementation.
In this manner, all the problem related to the shifting between synthetic and real environment, faced by deep learning algorithms, would be avoided. \\
The first outline of the algorithm was, hence, partially based on the Semi-Global Matching method \cite{Hirschmuller2008}. 
In order to analyse the suitability of the implementation, absolute computational time performances were not initially taken into account. 
At first it was chosen to focus on the relative efficiency that would be estimated among the different methods used in the principal pipeline.\\
As aforementioned in chapter \ref{chapter:environment}, these preliminary evaluations has been developed in MATLAB. 
In that phase an implementation sufficiently similar to the standard SGM method was designed. 
Thus, the focus was centred on the different types of algorithms used for matching the corresponding pixels and, then, on the part of the pipeline related to the matching and the aggregation costs, as precisely defined by Scharstein and Szeliski in \cite{Scharstein2001}.
The adoption of this working strategy was due to the following motivations. 
First of all, the predominant current researching aim of this project, whose purpose would most likely further shifts in more market based one, the need of test the effective performance in accuracy of pure SGM based method and the requirement of analyse the relative efficiency between that algorithm and a method, whose pipeline would be based on the information acquired exploiting the company device. \\
Considering that a fundamental part of this initial testing phase was focused on the matching cost evaluation, multiple matching algorithm has been tested in order to find the most efficient one mainly in terms of accuracy. 
Therefore, basing on the work done in \cite{Hirschmuller2007}, \cite{Patil2013} and \cite{Ko2017}, and making reference to the algorithm used for computing visual correspondence described in \cite{Zabih1994} and \cite{Demetz2013}, the performance of different matching cost algorithms has been analysed. 


\section{Deep-learning based methods}
\label{section:deep-learning-method}




\section{Pure derivative based data estimation algorithm}
\label{section:deriv-based-algorithm}

\section{SGM based data estimation algorithm}
\label{section:sgm-based-algorithm}


\section{Pre-processing techniques}
\label{section:pre-process-tech}

\section{Post-processing techniques}
\label{section:post-process-tech}

