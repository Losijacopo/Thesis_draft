\chapter{Introduction}
\label{chapter:intro}

The work developed in this Master's thesis is focused in a novel implementation of a stereo matching based procedure, which exploits, on top of a pair of stereo rectified images of the analysed scene, a sparse cloud of points generated by a multi-cameras device.
The device, designed by LaDiMo, the company in which this thesis has been developed, was actually employed to accelerate a standard stereo matching pipeline, thus to achieve real-time performance in 3D reconstruction. \\
Therefore, the main goal of this Master's thesis stand in the designing of an algorithm that allows to detect the object of an analysed scene, by increasing the density of an initial sparse cloud of point and exploiting a pair of rectified RGB stereo images.
As aforementioned, the main task of the utilization of the sparse 3D point cloud is to accelerate the stereo matching procedure, which would be otherwise slow and computationally expensive. \\
The final scopes of this algorithm can be theoretically multiple.
As a matter of fact, real-time object detection is fundamental in autonomous driving applications. 
The built depth estimation process can effectively facilitate warehouse managing operations.
Moreover the 3D scene reconstruction provided by the algorithm is crucial when dealing with automation and robotics, especially for indoor implementations.\\
Hence the actual final application in which the thought method can be employed are different.
However, the pivotal goal of this thesis is the design of an algorithm that provides an accurate dense 3D cloud of point, which describes the analysed scene.
Then, this would be employed for specific tasks, depending on the particular use case.

\section{Problem statement}
\label{sec:problem-statement}

Dense and accurate disparity maps are the key factor for obtaining correct depth estimations for many computer vision applications such as autonomous driving, 3D reconstruction, object detection and robotics.  
Therefore, stereo matching and disparity estimation can be identified as fundamental problems in the current developments of computer vision~\cite{Seki2017}.\\
Multiple methods for disparity estimation has been developed for many years~\cite{Scharstein2001}. 
Older strategies are focused on local-based or global-based methods. 
On the contrary, deep learning based strategies applied to local or global methods has been recently proposed. \\
The latter approach aims to a precise local correspondence exploiting deep learning and applying Semi-Global Matching (SGM) as the regularization step of the pipeline. 
Therefore, deep learning techniques such as FlowNet and DispNet~\cite{Seki2017} are used as the end-to-end part of the pipeline.
According to the current benchmark database ranks for stereo matching algorithms, e.g. the one published in the KITTI website\footnote{\url{http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo}}, the state of the art implementations are based on deep learning methods. 
However, these strategies lack in accuracy compared to the standard pipelines when dealing with scenes different from the ones used for training the network. 
Thus, this is probably due to the differences between real environment and the training database as underlined in~\cite{Seki2017} and \cite{Poggi2019}.\\
As aforementioned, the state of the art methods to recover dense disparity maps from stereo pairs are focused on deep Convolutional Neural Networks (CNNs) trained end-to-end~\cite{Tonioni2020}. 
Most of these techniques, which will be subsequently described, exploit as regularization phase the SGM method.\\
The Hirschm\"{u}ller's algorithm~\cite{Hirschmuller2008} is, in fact, used in both standard and deep learning based techniques because, considering the available benchmark data, its approach is still among the best performing in terms of computational cost and accuracy. 
For this reason, it is the preferred trade-off for most real-time applications, which are based on both the standard, local and global, and deep learning based strategies.\\
Therefore, as previously anticipated, the main problem, which this project is focused on, is the 3D reconstruction of a specific scene, obtained by depth data estimation.
The algorithm pipeline is based on a pair of stereo images,  necessary for the disparity calculation, which is then related to the depth of the specific pixel.
Additionally, crucial input to the method is the sparse cloud of point, which is exploited to accelerate the stereo matching procedure.\\
At the end of the pipeline, a 3D denser point cloud is basically obtained.
That cloud of point should be, thus, able to describe the object in the scene, making its identification simple. \\
Considering the stereo device designed by the company and hence the data available as input to the whole process, it has been decided since the beginning to develop the algorithm on standard method based strategy. 
Moreover, the choice of do not employing a deep learning based approach was also due to the so called \textit{domain shifting}~\cite{Poggi2019}.
This is defined as a dropping in accuracy when running the algorithm on a scene that present data different from the one used for training, which are typically synthetic data. 
Hence, the accessible device, the data that it can provide and the requirement of being able to apply the algorithm on whatever environment bring to the decision of basing the implementation on a standard approach.\\
Therefore, some initial tests have been carried out to obtain a rough estimation of the feasibility of the choice done.
Actually, the main aspect that has been researched was the potential improvement, primarily in terms of computational time, that the employment of the sparse point cloud could give to the whole pipeline.
This idea is similar to what has been proposed by Poggi et al. in their \textbf{\textit{Guided Stereo Matching}}~\cite{Poggi2019}, which is based on a deep learning based method, though.
Differently from the designed approach, they developed a method capable of reducing the lack in accuracy when shifting to unseen environments, by exploiting a set of sparse accurate depth measurements, which can be obtained from an type of source, such as LiDAR.\\
In this thesis work the problem of estimating a dense and reliable 3D point cloud, starting from a pair of rectified stereo images and a set of accurate depth measurements, has been tackled in a different manner.
Specifically, two main strategies have been designed and analysed.
A so called SGM-based method, which strongly follows the steps of the standard Semi-Global Matching algorithm to recover the disparity information.
Then, a novel approach has been designed.
It primarily employs the information that are available from the input point cloud.
The peculiar feature of this method stands in the calculation of the local derivatives among neighboring points of the cloud.
This can be defined as the key characteristic of the algorithm, which will be fully described in the following chapters.

\section{Structure of the Thesis}
\label{section:structure-of-thesis} 

Therefore, the two aforementioned stereo matching strategies and the whole presented work, which is going to be widely discussed in the next chapters, is now generally set out, thus to provide to the reader a generic understanding of the entire thesis project. \\ 
First of all, in Chapter~\ref{chapter:background} a theoretical background is delineated.\\
It will cover the relevant topics necessary to provide a complete understanding of the designed project.
Specifically, a wide analysis will involve the bases of most of the stereo matching methods.
Additionally, epipolar geometry and the meaning of \textit{rectification} are outlined, providing the relevant mathematical formulas, thus to support their explanation and correlation with the algorithms developed. \\
Moreover, a substantial part of the background is dedicated to the description of the standard and the deep learning based stereo matching algorithms, which have been precisely analysed through a complete literature review.\\
Then, at the end of Chapter~\ref{chapter:background} a section concerning the image processing techniques is proposed, being some those techniques exploited and tested during the pre and post-processing phases of the project.
Additionally, a section of that Chapter covers the most used edge detection and segmentation algorithms, which are going to be of particular interest for the future improvements of this project.\\
After the background chapter, the actual work is deeply outlined.
Chapter~\ref{chapter:environment} is related to the hardware and software environment employed.\\
Specifically, the general information on the datasets employed are set out and their specific use in the different phases of the project described.\\
Moreover, in Chapter\ref{chapter:environment} the LaDiMo device employed for recovering the real environment images and the initial point cloud is briefly described. 
In particular, its main functionalities and its hardware setup are outlined.\\
Then the methods implemented in the different approaches designed are displayed. \\
After that the specific implementation of the algorithms will take place. 
The actual structures of the different approaches designed are fully introduced and explained in detail.
At the end of that, in Chapter~\ref{chapter:evaluation} the results achieved are shown and commented.
The final discussion over the whole presented work is then outlined in Chapter~\ref{chapter:discussion}.
The main scope of that is to provide comments on the specific implementation of the algorithm and on the achieved results, which should provide an accomplishment of the initial goal set.\\
Thus, the entire work concludes with a summarize of the thesis, provided in Chapter~\ref{chapter:conclusions}

